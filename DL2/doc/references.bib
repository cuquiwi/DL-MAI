@article{Radford,
abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
file = {:home/senne/Documents/Mendeley Desktop/Radford et al/Unknown/Radford et al. - 2018 - Language Models are Unsupervised Multitask Learners.pdf:pdf},
mendeley-groups = {VAKKEN/DL/LAB 2},
title = {{Language Models are Unsupervised Multitask Learners}},
year = {2018}
}
@incollection{Goodfellow2014,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and Weinberger, K Q},
file = {:home/senne/Documents/Mendeley Desktop/Goodfellow et al/Advances in Neural Information Processing Systems 27/Goodfellow et al. - 2014 - Generative Adversarial Nets.pdf:pdf},
mendeley-groups = {VAKKEN/DL/LAB 2},
pages = {2672--2680},
publisher = {Curran Associates, Inc.},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@inproceedings{Gregor,
abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
archivePrefix = {arXiv},
arxivId = {1502.04623},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.04623},
file = {:home/senne/Documents/Mendeley Desktop/Gregor et al/32nd International Conference on Machine Learning, ICML 2015/Gregor et al. - 2015 - DRAW A recurrent neural network for image generation.pdf:pdf},
isbn = {9781510810587},
mendeley-groups = {VAKKEN/DL/LAB 2},
pages = {1462--1471},
title = {{DRAW: A recurrent neural network for image generation}},
volume = {2},
year = {2015}
}
@article{Hochreiter:1997:LSM:1246443.1246450,
address = {Cambridge, MA, USA},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Comput.},
mendeley-groups = {VAKKEN/DL/LAB 2},
month = {nov},
number = {8},
pages = {1735--1780},
publisher = {MIT Press},
title = {{Long Short-Term Memory}},
url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@techreport{Oord2016,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:home/senne/Documents/Mendeley Desktop/Oord et al/Unknown/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:pdf},
mendeley-groups = {VAKKEN/DL/LAB 2},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@inproceedings{VanDenOord2016,
abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two- dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNel dataset. Samples generated from the model appear crisp, varied and globally coherent.},
archivePrefix = {arXiv},
arxivId = {1601.06759},
author = {{Van Den Oord}, A{\"{a}}ron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
booktitle = {33rd International Conference on Machine Learning, ICML 2016},
eprint = {1601.06759},
file = {:home/senne/Documents/Mendeley Desktop/Van Den Oord, Kalchbrenner, Kavukcuoglu/33rd International Conference on Machine Learning, ICML 2016/Van Den Oord, Kalchbrenner, Kavukcuoglu - 2016 - Pixel recurrent neural networks.pdf:pdf},
isbn = {9781510829008},
mendeley-groups = {VAKKEN/DL/LAB 2},
pages = {2611--2620},
title = {{Pixel recurrent neural networks}},
volume = {4},
year = {2016}
}
@article{Alom2019,
abstract = {In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models.},
author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Hasan, Mahmudul and {Van Essen}, Brian C. and Awwal, Abdul A.S. and Asari, Vijayan K.},
doi = {10.3390/electronics8030292},
file = {:home/senne/Documents/Mendeley Desktop/Alom et al/Electronics (Switzerland)/Alom et al. - 2019 - A state-of-the-art survey on deep learning theory and architectures.pdf:pdf},
issn = {20799292},
journal = {Electronics (Switzerland)},
keywords = {Auto-encoder (AE),Convolutional neural network (CNN),Deep belief network (DBN),Deep learning,Deep reinforcement learning (DRL),Generative adversarial network (GAN),Recurrent neural network (RNN),Restricted Boltzmann machine (RBM),Transfer learning},
mendeley-groups = {VAKKEN/DL/LAB 2},
number = {3},
pages = {1--67},
title = {{A state-of-the-art survey on deep learning theory and architectures}},
volume = {8},
year = {2019}
}
